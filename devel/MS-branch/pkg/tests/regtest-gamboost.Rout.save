
R version 2.6.1 (2007-11-26)
Copyright (C) 2007 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> require("mboost")
Loading required package: mboost
Loading required package: modeltools
Loading required package: stats4
Loading required package: party
Loading required package: survival
Loading required package: splines

Attaching package: 'survival'


	The following object(s) are masked from package:modeltools :

	 cluster 

Loading required package: grid
Loading required package: coin
Loading required package: mvtnorm
Loading required package: zoo
Loading required package: sandwich
Loading required package: strucchange
Loading required package: vcd
Loading required package: MASS
Loading required package: colorspace
> 
> set.seed(290875)
> 
> ### for boosting hat matrix checks
> fm <- GaussReg()
> fm@offset <- function(y, w) 0
> 
> ### a simple two-dimensional example from `gamboost.Rd'
> data("cars")
> cars.gb <- gamboost(dist ~ speed, data = cars, df = 4, family = fm,
+                     control = boost_control(mstop = 50))
> cars.gb

	 Generalized Additive Models Fitted via Gradient Boosting

Call:
gamboost.formula(formula = dist ~ speed, data = cars, df = 4,     family = fm, control = boost_control(mstop = 50))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 50 
Step size:  0.1 
Offset:  0 
Degree of freedom:  4 

> aic <- AIC(cars.gb, method = "corrected")
> aic
[1] 6.605535
Optimal number of boosting iterations: 35 
Degrees of freedom (for mstop = 35): 5.117497 
> 
> ht <- hatvalues(cars.gb)
> 
> ### plot fit
> plot(dist ~ speed, data = cars)
> lines(cars$speed, predict(cars.gb[mstop(AIC(cars.gb))]), col = "red")
> lines(cars$speed, predict(smooth.spline(cars$speed, cars$dist), cars$speed)$y, 
+       col = "green")
> 
> #### check boosting hat matrix and subsetting / predict
> stopifnot(isTRUE(all.equal(drop(attr(ht, "hatmatrix") %*% cars$dist),
+                            as.vector(predict(cars.gb)))))
> ht25 <- hatvalues(cars.gb[25])
> stopifnot(isTRUE(all.equal(drop(attr(ht25, "hatmatrix") %*% cars$dist),
+                            as.vector(predict(cars.gb[25])))))
> stopifnot(isTRUE(all.equal(drop(attr(ht25, "hatmatrix") %*% cars$dist),
+                            as.vector(fitted(cars.gb[25])))))
> 
> ### check boosting hat matrix with multiple independent variables
> ### and weights
> data("bodyfat", package = "mboost")
> bffm <- DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth +
+       anthro3a + anthro3b + anthro3c + anthro4 - 1
> indep <- names(bodyfat)[names(bodyfat) != "DEXfat"]
> bodyfat[indep] <- lapply(bodyfat[indep], function(x) x - mean(x))
> bf_gam <- gamboost(bffm, data = bodyfat, control = boost_control(mstop = 10), 
+                    weights = runif(nrow(bodyfat)) * 10)
> ### aic <- AIC(bf_gam)
> ht <- hatvalues(bf_gam)
> 
> off <- bf_gam$offset
> u <- bf_gam$ustart
> 
> stopifnot(isTRUE(all.equal(drop(attr(ht, "hatmatrix") %*% u + off),
+                            as.vector(predict(bf_gam)))))
> stopifnot(isTRUE(all.equal(drop(attr(ht, "hatmatrix") %*% u + off),
+                            as.vector(fitted(bf_gam)))))
> 
> 
> ### compare `gamboost' with `lm' in cases where this is actually possible
> set.seed(290875)
> x <- matrix(runif(1000) * 10, ncol = 10)
> xf <- gl(4, nrow(x)/4)
> 
> ### OK, we need to allow for some small differences (larger mstop values
> ### would fix this)
> stopin <- function(x, y) stopifnot(max(abs(x - y)) < 0.1)
> 
> ### univariate linear model
> df <- data.frame(y = 3*x[,2], x = x)
> ga <- gamboost(y ~ x.2 - 1, data = df,
+                control = boost_control(mstop = 100, nu = 1))
> stopin(fitted(lm(y ~ x.2 - 1, data = df)), fitted(ga))
> 
> ### univariate model involving sin transformation
> df <- data.frame(y = sin(x[,1]), x = x)
> ga <- gamboost(y ~ x.1 - 1, data = df, 
+                control = boost_control(mstop = 100, nu = 1))
> stopin(fitted(lm(y ~ sin(x.1) - 1, data = df)), fitted(ga))
> 
> ### bivariate model: linear and sin
> df <- data.frame(y = sin(x[,1]) + 3*x[,2], x = x)
> ga <- gamboost(y ~ x.1 + x.2 - 1, data = df, 
+                control = boost_control(mstop = 100, nu = 1))
> stopin(fitted(lm(y ~ sin(x.1) + x.2 - 1, data = df)), fitted(ga))
> ga <- gamboost(y ~ x.1 + x.2 - 1, data = df, dfbase = c(4, 1), 
+                control = boost_control(mstop = 100, nu = 1))
> stopin(fitted(lm(y ~ sin(x.1) + x.2 - 1, data = df)), fitted(ga))
> 
> ### ANCOVA model
> df <- data.frame(y = 3 * x[,2] + (1:4)[xf], x = x)
> ga <- gamboost(y ~ xf + x.2 - 1, data = df, 
+                control = boost_control(mstop = 100, nu = 1))
> stopin(fitted(lm(y ~ xf + x.2 - 1, data = df)), fitted(ga))
> ga <- gamboost(y ~ xf + sin(x.1) + x.2, data = df, 
+                dfbase = c(1, 1, 4, 1),
+                control = boost_control(mstop = 100, nu = 1))
> stopin(fitted(lm(y ~ xf + sin(x.1) + x.2, data = df)), fitted(ga))
> 
> 
> ### check centering
> y <- rnorm(20)
> xn <- rnorm(20)
> xnm <- xn - mean(xn)
> xf <- gl(2, 10)
> gc <- gamboost(y ~ xn + xf, control = boost_control(center = TRUE))
Warning message:
In gamboost_fit(object, ...) : inputs are not centered in ‘gamboost’
> g <- gamboost(y ~ xnm + xf)
> cgc <- coef(gc)
> cg <- coef(g)  
> names(cgc) <- NULL
> names(cg) <- NULL 
> stopifnot(all.equal(cgc, cg))
> 
> pc1 <- predict(gc)
> pc2 <- predict(gc, newdata = data.frame(xn = xn, xf = xf))
> pc3 <- predict(g)
> stopifnot(all.equal(pc1, pc2))
> stopifnot(all.equal(pc2, pc3))
> 
> ### formula interfaces
> tmp <- data.frame(x1 = runif(100), x2 = runif(100), y = rnorm(100))
> fm1 <- y ~ bss(x1, df = 3) + bss(x2, df = 3)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
> mod2 <- gamboost(fm2, data = tmp, base = "bss", dfbase = 3)
> stopifnot(max(abs(fitted(mod1) - fitted(mod2))) < .Machine$double.eps)
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp))) < .Machine$double.eps)
> 
> fm1 <- y ~ bbs(x1, df = 3) + bbs(x2, df = 3)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
> mod2 <- gamboost(fm2, data = tmp, base = "bbs", dfbase = 3)
> stopifnot(max(abs(fitted(mod1) - fitted(mod2)))  < .Machine$double.eps)
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp)))  < .Machine$double.eps)
> 
> fm1 <- y ~ bols(x1) + bols(x2)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
> mod2 <- gamboost(fm2, data = tmp, base = "bols")
> stopifnot(max(abs(fitted(mod1) - fitted(mod2)))  < .Machine$double.eps)
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp)))  < .Machine$double.eps)
> 
> fm1 <- y ~ btree(x1) + btree(x2)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
> mod2 <- gamboost(fm2, data = tmp, base = "btree")
> stopifnot(max(abs(fitted(mod1) - fitted(mod2)))  < .Machine$double.eps)
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp)))  < .Machine$double.eps)
> 
> 
