
R version 3.1.1 (2014-07-10) -- "Sock it to Me"
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> require("mboostDevel")
Loading required package: mboostDevel
Loading required package: parallel
Loading required package: stabs
This is mboostDevel 2.4-0. See 'package?mboostDevel' and the NEWS file
for a complete list of changes.

> require("survival")
Loading required package: survival
Loading required package: splines
> 
> set.seed(290875)
> 
> ### for boosting hat matrix checks
> fm <- Gaussian()
> fm@offset <- function(y, w) 0
> 
> ### a simple two-dimensional example from `gamboost.Rd'
> data("cars")
> cars.gb <- gamboost(dist ~ speed, data = cars, df = 4, family = fm,
+                     control = boost_control(mstop = 50))
> cars.gb

	 Model-based Boosting

Call:
gamboost(formula = dist ~ speed, data = cars, dfbase = 4, family = fm,     control = boost_control(mstop = 50))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 50 
Step size:  0.1 
Offset:  0 
Number of baselearners:  1 

> aic <- AIC(cars.gb, method = "corrected")
> aic
[1] 6.58661
Optimal number of boosting iterations: 37 
Degrees of freedom (for mstop = 37): 4.37119 
> 
> ht <- hatvalues(cars.gb)
> 
> ### plot fit
> plot(dist ~ speed, data = cars)
> lines(cars$speed, predict(cars.gb[mstop(AIC(cars.gb))]), col = "red")
> lines(cars$speed, predict(smooth.spline(cars$speed, cars$dist), cars$speed)$y,
+       col = "green")
> 
> #### check boosting hat matrix and subsetting / predict
> stopifnot(isTRUE(all.equal(drop(attr(ht, "hatmatrix") %*% cars$dist),
+                            as.vector(predict(cars.gb[50])))))
> ht25 <- hatvalues(cars.gb[25])
> stopifnot(isTRUE(all.equal(drop(attr(ht25, "hatmatrix") %*% cars$dist),
+                            as.vector(predict(cars.gb[25])))))
> stopifnot(isTRUE(all.equal(drop(attr(ht25, "hatmatrix") %*% cars$dist),
+                            as.vector(fitted(cars.gb[25])))))
> 
> ### check boosting hat matrix with multiple independent variables
> ### and weights
> data("bodyfat", package = "TH.data")
> bffm <- DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth +
+       anthro3a + anthro3b + anthro3c + anthro4
> indep <- names(bodyfat)[names(bodyfat) != "DEXfat"]
> bodyfat[indep] <- lapply(bodyfat[indep], function(x) x - mean(x))
> bf_gam <- gamboost(bffm, data = bodyfat, control = boost_control(mstop = 10),
+                    weights = runif(nrow(bodyfat)) * 10)
> ### aic <- AIC(bf_gam)
> ht <- hatvalues(bf_gam)
> 
> off <- bf_gam$offset
> u <- bf_gam$ustart
> 
> stopifnot(isTRUE(all.equal(drop(attr(ht, "hatmatrix") %*% u + off),
+                            as.vector(predict(bf_gam)))))
> stopifnot(isTRUE(all.equal(drop(attr(ht, "hatmatrix") %*% u + off),
+                            as.vector(fitted(bf_gam)))))
> 
> 
> ### compare `gamboost' with `lm' in cases where this is actually possible
> set.seed(290875)
> x <- matrix(runif(1000) * 10, ncol = 10)
> xf <- gl(4, nrow(x)/4)
> 
> ### OK, we need to allow for some small differences (larger mstop values
> ### would fix this)
> stopin <- function(x, y) stopifnot(max(abs(x - y)) < 0.1)
> 
> ### univariate linear model
> df <- data.frame(y = 3*x[,2], x = x)
> ga <- gamboost(y ~ x.2, data = df,
+                control = boost_control(mstop = 500, nu = 1))
> stopin(fitted(lm(y ~ x.2 - 1, data = df)), fitted(ga))
> 
> ### univariate model involving sin transformation
> df <- data.frame(y = sin(x[,1]), x = x)
> ga <- gamboost(y ~ x.1, data = df,
+                control = boost_control(mstop = 500, nu = 1))
> stopin(fitted(lm(y ~ sin(x.1) - 1, data = df)), fitted(ga))
> 
> ### bivariate model: linear and sin
> df <- data.frame(y = sin(x[,1]) + 3*x[,2], x = x)
> ga <- gamboost(y ~ x.1 + x.2, data = df,
+                control = boost_control(mstop = 500, nu = 1))
> stopin(fitted(lm(y ~ sin(x.1) + x.2 - 1, data = df)), fitted(ga))
> ga <- gamboost(y ~ x.1 + bols(x.2), data = df,
+                control = boost_control(mstop = 500, nu = 1))
> stopin(fitted(lm(y ~ sin(x.1) + x.2 - 1, data = df)), fitted(ga))
> 
> ### ANCOVA model
> df <- data.frame(y = 3 * x[,2] + (1:4)[xf], x = x)
> ga <- gamboost(y ~ xf + x.2, data = df,
+                control = boost_control(mstop = 500, nu = 1))
Warning message:
In bbs(as.data.frame(list(...)), df = dfbase) :
  cannot compute 'bbs' for non-numeric variables; used 'bols' instead.
> stopin(fitted(lm(y ~ xf + x.2 - 1, data = df)), fitted(ga))
> ga <- gamboost(y ~ xf + sin(x.1) + x.2, data = df,
+                control = boost_control(mstop = 500, nu = 1))
Warning message:
In bbs(as.data.frame(list(...)), df = dfbase) :
  cannot compute 'bbs' for non-numeric variables; used 'bols' instead.
> stopin(fitted(lm(y ~ xf + sin(x.1) + x.2, data = df)), fitted(ga))
> 
> ### check centering
> y <- rnorm(200)
> xn <- rnorm(200)
> xnm <- xn - mean(xn)
> xf <- gl(2, 100)
> gc <- gamboost(y ~ xn + xf)
Warning message:
In bbs(as.data.frame(list(...)), df = dfbase) :
  cannot compute 'bbs' for non-numeric variables; used 'bols' instead.
> g <- gamboost(y ~ xnm + xf)
Warning message:
In bbs(as.data.frame(list(...)), df = dfbase) :
  cannot compute 'bbs' for non-numeric variables; used 'bols' instead.
> stopifnot(max(abs(fitted(gc) - fitted(g))) < 1 / 10000)
> 
> pc1 <- predict(gc)
> pc2 <- predict(gc, newdata = data.frame(xn = xn, xf = xf))
> pc3 <- predict(g)
> stopifnot(all.equal(pc1, pc2))
> stopifnot(max(abs(pc2 - pc3)) < 1 / 10000)
> 
> ### formula interfaces
> tmp <- data.frame(x1 = runif(100), x2 = runif(100), y = rnorm(100))
> fm1 <- y ~ bbs(x1, df = 3) + bbs(x2, df = 3)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
> mod2 <- gamboost(fm2, data = tmp, base = "bss", dfbase = 3)
Warning message:
In gamboost(fm2, data = tmp, base = "bss", dfbase = 3) :
  bss and bns are deprecated, bbs is used instead
> stopifnot(max(abs(fitted(mod1) - fitted(mod2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp))) < sqrt(.Machine$double.eps))
> 
> fm1 <- y ~ bbs(x1, df = 3) + bbs(x2, df = 3)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
> mod2 <- gamboost(fm2, data = tmp, base = "bbs", dfbase = 3)
> stopifnot(max(abs(fitted(mod1) - fitted(mod2)))  < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp)))  < sqrt(.Machine$double.eps))
> 
> fm1 <- y ~ bols(x1) + bols(x2)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
> mod2 <- gamboost(fm2, data = tmp, base = "bols")
> stopifnot(max(abs(fitted(mod1) - fitted(mod2)))  < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp)))  < sqrt(.Machine$double.eps))
> 
> fm1 <- y ~ btree(x1) + btree(x2)
> fm2 <- y ~ x1 + x2
> mod1 <- gamboost(fm1, data = tmp)
Loading required package: party
Loading required package: grid
Loading required package: zoo

Attaching package: 'zoo'

The following objects are masked from 'package:base':

    as.Date, as.Date.numeric

Loading required package: sandwich
Loading required package: strucchange
Loading required package: modeltools
Loading required package: stats4
> mod2 <- gamboost(fm2, data = tmp, base = "btree")
> stopifnot(max(abs(fitted(mod1) - fitted(mod2)))  < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(mod1, newdata = tmp) - predict(mod2, newdata = tmp)))  < sqrt(.Machine$double.eps))
> 
> ## Cox model
> 
> fit2 <- gamboost(Surv(futime, fustat) ~ bbs(age) +
+     bols(resid.ds) + bols(rx) + bols(ecog.ps), data = ovarian,
+     family = CoxPH(), control = boost_control(mstop = 1000))
> 
> A2 <- survFit(fit2)
> A2
$surv
           [,1]
 [1,] 0.9952287
 [2,] 0.9882014
 [3,] 0.9791706
 [4,] 0.9671912
 [5,] 0.9115227
 [6,] 0.8581761
 [7,] 0.8026945
 [8,] 0.7316060
 [9,] 0.6567913
[10,] 0.5825302
[11,] 0.4737687
[12,] 0.3723020

$time
 [1]  59 115 156 268 329 353 365 431 464 475 563 638

$n.event
 [1] 1 1 1 1 1 1 1 1 1 1 1 1

attr(,"class")
[1] "survFit"
> 
> newdata <- ovarian[c(1,3,12),]
> A2 <- survFit(fit2, newdata = newdata)
> A2
$surv
                 1            3        12
 [1,] 7.224865e-01 8.759679e-01 0.9975414
 [2,] 4.463454e-01 7.199103e-01 0.9939098
 [3,] 2.391600e-01 5.583185e-01 0.9892243
 [4,] 1.035965e-01 3.970621e-01 0.9829766
 [5,] 1.843298e-03 7.691705e-02 0.9534377
 [6,] 3.058580e-05 1.448212e-02 0.9242974
 [7,] 3.256537e-07 2.275840e-03 0.8930422
 [8,] 5.964830e-10 1.745993e-04 0.8514192
 [9,] 3.903423e-13 8.807263e-06 0.8054337
[10,] 1.121666e-16 3.177056e-07 0.7571977
[11,] 8.909567e-23 1.039739e-09 0.6807910
[12,] 6.853145e-30 1.314449e-12 0.6013670

$time
 [1]  59 115 156 268 329 353 365 431 464 475 563 638

$n.event
 [1] 1 1 1 1 1 1 1 1 1 1 1 1

attr(,"class")
[1] "survFit"
> 
> ### gamboost with explicit intercept
> df <- data.frame(x = 1:100, y = rnorm(1:100), int = rep(1, 100))
> mod <- gamboost(y ~ bols(int, intercept = FALSE) + bols(x, intercept = FALSE), data = df,
+                 control = boost_control(mstop = 2500))
Warning message:
In bols(x, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
> cf <- unlist(coef(mod))
> cf[1] <- cf[1] + mod$offset
> tmp <- max(abs(cf - coef(lm(y ~ x, data = df))))
> stopifnot(tmp < 1e-5)
> tmp <- max(abs(fitted(mod) - fitted(lm(y ~ x, data = df))))
> stopifnot(tmp < 1e-5)
> 
> ### predictions:
> data("bodyfat", package = "TH.data")
> amod <- gamboost(DEXfat ~ hipcirc + anthro3a, data = bodyfat, baselearner = "bbs")
> 
> agg <- c("none", "sum", "cumsum")
> whi <- list(NULL, 1, 2, c(1,2))
> for (i in 1:4){
+     pred <- vector("list", length=3)
+     for (j in 1:3){
+         pred[[j]] <- predict(amod, aggregate=agg[j], which = whi[[i]])
+     }
+     if (i == 1){
+         stopifnot(max(abs(pred[[2]] - pred[[3]][,ncol(pred[[3]])]))  < sqrt(.Machine$double.eps))
+         if ((pred[[2]] - rowSums(pred[[1]]))[1] - attr(coef(amod), "offset") < sqrt(.Machine$double.eps))
+             warning(sQuote("aggregate = sum"), " adds the offset, ", sQuote("aggregate = none"), " doesn't.")
+         stopifnot(max(abs(pred[[2]] - rowSums(pred[[1]]) - attr(coef(amod), "offset")))   < sqrt(.Machine$double.eps))
+     } else {
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[3]], function(obj) obj[,ncol(obj)])))  < sqrt(.Machine$double.eps))
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[1]], function(obj) rowSums(obj))))  < sqrt(.Machine$double.eps))
+     }
+ }
Warning message:
'aggregate = sum' adds the offset, 'aggregate = none' doesn't. 
> 
> # use names in which instead of numbers
> agg <- c("none", "sum", "cumsum")
> whi <- list(NULL, "hipcirc", "anthro3a", c("hipcirc", "anthro3a"))
> for (i in 1:4){
+     pred <- vector("list", length=3)
+     for (j in 1:3){
+         pred[[j]] <- predict(amod, aggregate=agg[j], which = whi[[i]])
+     }
+     if (i == 1){
+         stopifnot(max(abs(pred[[2]] - pred[[3]][,ncol(pred[[3]])]))  < sqrt(.Machine$double.eps))
+         if ((pred[[2]] - rowSums(pred[[1]]))[1] - attr(coef(amod), "offset") < sqrt(.Machine$double.eps))
+             warning(sQuote("aggregate = sum"), " adds the offset, ", sQuote("aggregate = none"), " doesn't.")
+         stopifnot(max(abs(pred[[2]] - rowSums(pred[[1]]) - attr(coef(amod), "offset")))   < sqrt(.Machine$double.eps))
+     } else {
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[3]], function(obj) obj[,ncol(obj)])))  < sqrt(.Machine$double.eps))
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[1]], function(obj) rowSums(obj))))  < sqrt(.Machine$double.eps))
+     }
+ }
Warning message:
'aggregate = sum' adds the offset, 'aggregate = none' doesn't. 
> 
> # use which to extract all effects for one covariate e.g. for plotting purposes
> amod <- gamboost(DEXfat ~ bols(hipcirc, intercept=FALSE) + bbs(hipcirc, df = 1, center=TRUE), data = bodyfat)
Warning message:
In bols(hipcirc, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
> stopifnot(ncol(predict(amod, which="hip")) == 2 && all(rowSums(predict(amod, which="hip")) + attr(coef(amod), "offset") - predict(amod) < sqrt(.Machine$double.eps)))
> 
> amod <- gamboost(DEXfat ~ hipcirc + anthro3a + kneebreadth,
+                  data = bodyfat, baselearner = "bbs")
> pr1 <- predict(amod, aggre = "sum", which= 1:2)
> foo <- bodyfat[,names(bodyfat) %in% c("hipcirc", "anthro3a", "kneebreadth")]
> foo$kneebreadth <- mean(bodyfat$kneebreadth)
> pr2 <- predict(amod, aggre = "sum", newdata=foo)
> stopifnot(all(diff(rowSums(pr1) - pr2) < sqrt(.Machine$double.eps))) # changes in level are ok
> newData <- as.data.frame(rbind(colMeans(bodyfat)[-2], colMeans(bodyfat)[-2]+1*sapply(bodyfat, sd)[-2]))
> if (!is.list(pr <- predict(amod, newdata=newData, which=1:2)))
+     warning("predict(amod, newdata=newData, which=1:2) does not return a list") # no list but a matrix is returned!
Warning message:
predict(amod, newdata=newData, which=1:2) does not return a list 
> stopifnot(is.list(pr <- predict(amod, newdata=newData, aggregate="cumsum", which=1:2)))
> amod[10]

	 Model-based Boosting

Call:
gamboost(formula = DEXfat ~ hipcirc + anthro3a + kneebreadth,     data = bodyfat, baselearner = "bbs")


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 10 
Step size:  0.1 
Offset:  30.78282 
Number of baselearners:  3 

> pr <- predict(amod, which=1:3)
> stopifnot(ncol(pr) == 3 || all(pr[,ncol] == 0))
> amod[100]

	 Model-based Boosting

Call:
gamboost(formula = DEXfat ~ hipcirc + anthro3a + kneebreadth,     data = bodyfat, baselearner = "bbs")


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  30.78282 
Number of baselearners:  3 

> 
> # check type argument
> set.seed(1907)
> x1 <- rnorm(100)
> p <- 1/(1 + exp(- 3 * x1))
> y <- as.factor(runif(100) < p)
> DF <- data.frame(y = y, x1 = x1)
> 
> logitBoost <- gamboost(y ~ x1, family = Binomial(),
+                  data = DF, baselearner = "bols", control=boost_control(mstop=5000))
> logit <- glm(y ~ x1, data=DF, family=binomial)
> stopifnot(coef(logitBoost)[[1]][2]*2 - coef(logit)[2]  < sqrt(.Machine$double.eps)) # * 2 as we use y = {-1, 1}

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

> 
> pr <- predict(logitBoost)
> pr2 <- predict(logit)
> stopifnot(pr * 2 - pr2 < 1e-5)  # * 2 as we use y = {-1, 1}
> 
> pr <- predict(logitBoost, type="class")
> pr2 <- predict(logit, type="response") > 0.5
> foo <- table(pr, pr2)
> stopifnot(foo[1,2] + foo[2,1] == 0)
> 
> pr <- predict(logitBoost, type="response")
> pr2 <- predict(logit, type="response")
> stopifnot(pr - pr2  < sqrt(.Machine$double.eps))
> 
> ### coefficients:
> data("bodyfat", package = "TH.data")
> amod <- gamboost(DEXfat ~ hipcirc + anthro3a + kneebreadth,
+                  data = bodyfat, baselearner = "bbs")
> stopifnot(length(coef(amod)) == 3)
> amod[10]

	 Model-based Boosting

Call:
gamboost(formula = DEXfat ~ hipcirc + anthro3a + kneebreadth,     data = bodyfat, baselearner = "bbs")


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 10 
Step size:  0.1 
Offset:  30.78282 
Number of baselearners:  3 

> stopifnot(length(coef(amod)) == 2)
> stopifnot(length(coef(amod, which=1:3)) == 3)
> 
> ### cyclic covariates
> x <- seq(from = 0, to = 2*pi, length = 100)
> y <- sin(x) + rnorm(length(x), sd = 0.5)
> mod <- gamboost(y ~ bbs(x, cyclic = TRUE))
> stopifnot(diff(fitted(mod)[c(1, 100)]) == 0)
> 
> ### buser
> set.seed(1907)
> x <- rnorm(100)
> y <- rnorm(100, mean = x^2, sd = 0.1)
> mod1 <- gamboost(y ~ bbs(x))
> X <- extract(bbs(x))
> K <- extract(bbs(x), "penalty")
> mod2 <- gamboost(y ~ buser(X, K))
> stopifnot(max(abs(predict(mod1) - predict(mod2))) < sqrt(.Machine$double.eps))
> 
> z <- sample(1:2, 100, replace=TRUE)
> y[z == 2] <- rnorm(100, mean = - x^2, sd = 0.1)[z == 2]
> z <- as.factor(z)
> mod3 <- gamboost(y ~ bbs(x) + bbs(x, by = z),
+                  control = boost_control(mstop = 1000))
> X <- extract(bbs(x))
> K <- extract(bbs(x), "penalty")
> mod4 <- gamboost(y ~  buser(X, K) + buser(X, K, by = z),
+                  control = boost_control(mstop = 1000))
> stopifnot(max(abs(predict(mod3) - predict(mod4))) < sqrt(.Machine$double.eps))
> 
> y <- rnorm(100, mean = as.numeric(z), sd = 0.1)
> mod5 <- gamboost(y ~ bols(z))
> X <- extract(bols(z))
> K <- extract(bols(z), "penalty")
> index <- extract(bols(z), "index")
> mod6 <- gamboost(y ~  buser(X, K, lambda = 0, index = index))
> mod6a <- gamboost(y ~  buser(X, index = index))
> stopifnot(max(abs(predict(mod5) - predict(mod6))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(mod5) - predict(mod6a))) < sqrt(.Machine$double.eps))
> 
> z <- sample(1:3, 100, replace = TRUE)
> y <- rnorm(100, mean = z, sd = 0.1)
> z <- as.ordered(z)
> mod7 <- gamboost(y ~ bols(z))
> X <- extract(bols(z))
> K <- extract(bols(z), "penalty")
> index <- extract(bols(z), "index")
> mod8 <- gamboost(y ~  buser(X, K, lambda = 0, index = index))
> stopifnot(max(abs(predict(mod7) - predict(mod8))) < sqrt(.Machine$double.eps))
> 
> y[z == 1] <- rnorm(100, mean = x^2, sd = 0.1)[z == 1]
> y[z == 2] <- rnorm(100, mean = - x^2, sd = 0.1)[z == 2]
> y[z == 3] <- rnorm(100, mean = x, sd = 0.1)[z == 3]
> mod9 <- gamboost(y ~ bbs(x) + bbs(x, by = z),
+                  control = boost_control(mstop = 1000))
Warning in Xfun(mf, vary, args) :
  'df' equal to rank of null space (unpenalized part of P-spline);
  Consider larger value for 'df' or set 'center != FALSE'.
> X <- extract(bbs(x))
> K <- extract(bbs(x), "penalty")
> mod10 <- gamboost(y ~  buser(X, K) + buser(X, K, by = z),
+                  control = boost_control(mstop = 1000))
> stopifnot(max(abs(predict(mod9) - predict(mod10))) < sqrt(.Machine$double.eps))
> 
> proc.time()
   user  system elapsed 
 11.110   0.250  11.664 
