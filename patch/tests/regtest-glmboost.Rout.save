
R version 2.14.0 (2011-10-31)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> require("mboost")
Loading required package: mboost
> 
> set.seed(290875)
> 
> dgp <- function(n = 100, beta = rep(0, 10), sd = 1) {
+     p <- length(beta) - 1
+     x <- cbind(1, matrix(runif(n * p), ncol = p))
+     lp <- x %*% beta
+     y <- lp + rnorm(n, sd = sd)
+     ls <- data.frame(y = y, x[,-1])
+     attr(ls, "lp") <- lp
+     ls
+ }
> 
> ### well-defined problem
> mydf <- dgp(beta = c(1, 2.5, rep(0, 2)))
> 
> ### for easy comparison with lm
> fm <- Gaussian()
> fm@offset <- function(y, w) 0
> 
> mydf.gb <- glmboost(y ~ ., data = mydf, family = fm,
+                     control = boost_control(mstop = 1000, nu = 1))
> mydf.lm <- lm(y ~ ., data = mydf)
> 
> ### compare coefficients
> stopifnot(max(abs(coef(mydf.gb) - coef(mydf.lm))) < 1e-10)
> 
> ### a little bit more difficult
> mydf <- dgp(beta = c(1, 2.5, rep(0, 38)))
> 
> mydf.gb <- glmboost(y ~ ., data = mydf, family = fm,
+                     control = boost_control(mstop = 1000, nu = 1))
> aic <- AIC(mydf.gb, method = "corrected")
> ht <- hatvalues(mydf.gb)
> mstop(aic)
[1] 12
> mydf.lm <- lm(y ~ ., data = mydf)
> 
> ### compare coefficients
> which(abs(coef(mydf.lm)) < abs(coef(mydf.gb[mstop(aic)])))
(Intercept)          X1          X2          X4          X5          X6 
          1           2           3           5           6           7 
         X7          X8          X9         X11         X12         X13 
          8           9          10          12          13          14 
        X14         X15         X16         X17         X19         X20 
         15          16          17          18          20          21 
        X21         X22         X23         X24         X25         X27 
         22          23          24          25          26          28 
        X29         X30         X31         X33         X34         X35 
         30          31          32          34          35          36 
        X37         X38         X39 
         38          39          40 
Warning message:
In abs(coef(mydf.lm)) < abs(coef(mydf.gb[mstop(aic)])) :
  longer object length is not a multiple of shorter object length
> 
> #### check boosting hat matrix and subsetting / predict
> stopifnot(isTRUE(all.equal(drop(attr(ht, "hatmatrix") %*% mydf$y),
+                            as.vector(predict(mydf.gb[1000])))))
> ht25 <- hatvalues(mydf.gb[25])
> stopifnot(isTRUE(all.equal(drop(attr(ht25, "hatmatrix") %*% mydf$y),
+                            as.vector(predict(mydf.gb[25])))))
> stopifnot(isTRUE(all.equal(drop(attr(ht25, "hatmatrix") %*% mydf$y),
+                            as.vector(fitted(mydf.gb[25])))))
> 
> ### a simple two-dimensional example from `glmboost.Rd'
> data("cars")
> cars.gb <- glmboost(dist ~ speed, data = cars, family = fm, center = FALSE,
+                     control = boost_control(mstop = 1000, nu = 1))
> cars.gb

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = dist ~ speed, data = cars, center = FALSE,     control = boost_control(mstop = 1000, nu = 1), family = fm)


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 1000 
Step size:  1 
Offset:  0 

Coefficients: 
(Intercept)       speed 
 -17.579095    3.932409 
attr(,"offset")
[1] 0

> 
> ### coefficients should coincide
> cf <- coef(cars.gb)
> attr(cf, "offset") <- NULL
> stopifnot(all.equal(cf, coef(lm(dist ~ speed, data = cars))))
> 
> ### logistic regression
> mydf <- data.frame(x = runif(100), z = rnorm(100),
+                    y = factor(c(rep(0, 30), rep(1, 70))))
> bmod <- glmboost(y ~ x + z, data = mydf, family = Binomial(), center = FALSE,
+                  control = boost_control(mstop = 1000, nu = 1))
> gmod <- glm(y ~ x + z, data = mydf, family = binomial())
> llg <- logLik(gmod)
> attributes(llg) <- NULL
> stopifnot(all.equal(logLik(bmod), llg))
> stopifnot(max(abs(predict(gmod, type = "link")/2 - fitted(bmod))) <
+                   sqrt(.Machine$double.eps))
> cfb <- coef(bmod, off2int = TRUE) * 2
> stopifnot(all.equal(cfb, coef(gmod)))
> aic <- AIC(bmod, "classical")
> stopifnot(abs(AIC(gmod) - attr(aic, "AIC")[mstop(bmod)]) < 1e-5)
> 
> ### weighted least squares problem
> 
> x <- runif(100)
> df <- data.frame(y = 2 + 3 * x + rnorm(length(x)),
+                  x = x, z = runif(length(x)),
+                  w = runif(length(x)) * 10)
> 
> ### linear model, classical fit
> lmmod <- lm(y ~ x + z, data = df, weights = w)
> 
> ### linear model, boosting fit
> lmb <- glmboost(y ~ x + z, data = df, weights = df$w, center = FALSE,
+                 control = boost_control(mstop = 5000, nu = 1))
> 
> ### compare fitted values
> stopifnot(max(abs(fitted(lmmod) -fitted(lmb))) < sqrt(.Machine$double.eps))
> 
> ### compare hat matrices
> stopifnot(max(abs(hatvalues(lmmod) - hatvalues(lmb))) < sqrt(.Machine$double.eps))
> 
> ### compare boosting hat matrix with fitted values
> stopifnot(max(abs(attr(hatvalues(lmb), "hatmatrix") %*% (df$y - lmb$offset) + lmb$offset -
+         fitted(lmb))) < sqrt(.Machine$double.eps))
> 
> ### Cox model (check for CoxPH family)
> if (require("survival")) {
+ 
+     test <- data.frame(time = c(1, 2, 5, 2, 1, 7, 3, 4, 8, 8),
+                        event = c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0),
+                        x     = c(1, 0, 0, 1, 0, 1, 1, 1, 0, 0))
+ 
+     stopifnot(all.equal(coef(cx <- coxph(Surv(time, event) ~ x, data = test, method = "breslow")),
+                        coef(gl <- glmboost(Surv(time, event) ~ x, data = test,
+                        family = CoxPH(), center = FALSE,
+                        control = boost_control(mstop = 2000, nu = 1)), which = 1:2)[2]))
+ 
+     stopifnot(all.equal(cx$loglik[2], logLik(gl)))
+ 
+     indx <- c(1, 1, 1, 2:10)
+     w <- tabulate(indx)
+ 
+     stopifnot(all.equal(coef(cx <- coxph(Surv(time, event) ~ x, data = test, weights = w,
+                                    method = "breslow")),
+                        coef(gl <- glmboost(Surv(time, event) ~ x, data = test, weights = w,
+                        family = CoxPH(), center = FALSE,
+                        control = boost_control(mstop = 200, nu = 1)), which = 1:2)[2]))
+ 
+     stopifnot(all.equal(cx$loglik[2], logLik(gl)))
+ 
+     indx <- c(1, 1, 1, 3:10)
+     w <- tabulate(indx)
+ 
+     stopifnot(all.equal(coef(cx <- coxph(Surv(time, event) ~ x, data = test[indx,],
+                                    method = "breslow")),
+                        coef(gl <- glmboost(Surv(time, event) ~ x, data = test, weights = w,
+                        family = CoxPH(), center = FALSE,
+                        control = boost_control(mstop = 1000)), which = 1:2)[2], tolerance = .Machine$double.eps ^ 0.125))
+ 
+     stopifnot(all.equal(cx$loglik[2], logLik(gl)))
+ }
Loading required package: survival
Loading required package: splines
> 
> 
> ## Cox model with predictions obtained from survFit function
> 
> fm <- Surv(futime,fustat) ~ age + resid.ds + rx + ecog.ps - 1
> ## no need to drop the intercept here:
> fmSurv <- Surv(futime,fustat) ~ age + resid.ds + rx + ecog.ps
> fit <- coxph(fmSurv, data = ovarian)
> fit2 <- glmboost(fm, data = ovarian, family = CoxPH(),
+     control=boost_control(mstop = 1000), center = TRUE)
Warning messages:
1: In glmboost.formula(fm, data = ovarian, family = CoxPH(), control = boost_control(mstop = 1000),  :
  model with centered covariates does not contain intercept
2: In optimize(risk, interval = c(0, max(y[, 1], na.rm = TRUE)), y = y,  :
  NA/Inf replaced by maximum positive value
> fit3 <- glmboost(fm, data = ovarian, family = CoxPH(),
+     control=boost_control(mstop = 1000), center = FALSE)
Warning message:
In optimize(risk, interval = c(0, max(y[, 1], na.rm = TRUE)), y = y,  :
  NA/Inf replaced by maximum positive value
> 
> A1 <- survfit(fit, censor = FALSE)
> A2 <- survFit(fit2)
> A3 <- survFit(fit3)
> 
> max(A1$surv-A2$surv)
[1] 4.991776e-05
> max(A1$surv-A3$surv)
[1] 0.07803166
> 
> newdata <- ovarian[c(1,3,12),]
> A1 <- survfit(fit, newdata = newdata, censor = FALSE)
> A2 <- survFit(fit2, newdata = newdata)
> A3 <- survFit(fit3, newdata = newdata)
> 
> max(A1$surv-A2$surv)
[1] 8.23777e-05
> max(A1$surv-A3$surv)
[1] 0.1886635
> 
> ### Poisson models
> 
> df <- data.frame(x1 = runif(100), x2 = runif(100))
> f <- -1 + 3 * df$x2
> df$y <- round(exp(f) )
> ctrl <- boost_control(mstop = 2000, nu = 0.1)
> 
> gmod <- glm(y ~ x1 + x2, data = df, family = poisson())
> gbmod <- glmboost(y ~ x1 + x2, data = df, family = Poisson(), control = ctrl,
+                   center = FALSE)
> 
> llg <- logLik(gmod)
> attributes(llg) <- NULL
> stopifnot(all.equal(logLik(gbmod), llg))
> 
> ### hat matrix is only approximate!
> stopifnot(abs(AIC(gmod) - attr(AIC(gbmod, "classical"), "AIC")[mstop(gbmod)]) < 1)
> 
> stopifnot(max(abs(predict(gmod) -  predict(gbmod))) < 1e-4)
> 
> ### predictions:
> set.seed(1907)
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> x3 <- rnorm(100)
> y <- rnorm(100, mean = 3 * x1, sd = 2)
> DF <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)
> 
> amod <- glmboost(y ~ -1 + x1 + x2, data = DF, center = FALSE)
> agg <- c("none", "sum", "cumsum")
> whi <- list(NULL, 1, 2, c(1,2))
> for (i in 1:4){
+     pred <- vector("list", length=3)
+     for (j in 1:3){
+         pred[[j]] <- predict(amod, aggregate=agg[j], which = whi[[i]])
+     }
+     if (i == 1){
+         stopifnot(max(abs(pred[[2]] - pred[[3]][,ncol(pred[[3]])]))  < sqrt(.Machine$double.eps))
+         if ((pred[[2]] - rowSums(pred[[1]]))[1] - attr(coef(amod), "offset") < sqrt(.Machine$double.eps))
+             warning(sQuote("aggregate = sum"), " adds the offset, ", sQuote("aggregate = none"), " doesn't.")
+         stopifnot(max(abs(pred[[2]] - rowSums(pred[[1]]) - attr(coef(amod), "offset")))   < sqrt(.Machine$double.eps))
+     } else {
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[3]], function(obj) obj[,ncol(obj)])))  < sqrt(.Machine$double.eps))
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[1]], function(obj) rowSums(obj))))  < sqrt(.Machine$double.eps))
+     }
+ }
Warning message:
'aggregate = sum' adds the offset, 'aggregate = none' doesn't. 
> 
> agg <- c("none", "sum", "cumsum")
> whi <- list(NULL, "x1", "x2", c("x1","x2"))
> for (i in 1:4){
+     pred <- vector("list", length=3)
+     for (j in 1:3){
+         pred[[j]] <- predict(amod, aggregate=agg[j], which = whi[[i]])
+     }
+     if (i == 1){
+         stopifnot(max(abs(pred[[2]] - pred[[3]][,ncol(pred[[3]])]))  < sqrt(.Machine$double.eps))
+         if ((pred[[2]] - rowSums(pred[[1]]))[1] - attr(coef(amod), "offset") < sqrt(.Machine$double.eps))
+             warning(sQuote("aggregate = sum"), " adds the offset, ", sQuote("aggregate = none"), " doesn't.")
+         stopifnot(max(abs(pred[[2]] - rowSums(pred[[1]]) - attr(coef(amod), "offset")))   < sqrt(.Machine$double.eps))
+     } else {
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[3]], function(obj) obj[,ncol(obj)])))  < sqrt(.Machine$double.eps))
+         stopifnot(max(abs(pred[[2]] - sapply(pred[[1]], function(obj) rowSums(obj))))  < sqrt(.Machine$double.eps))
+     }
+ }
Warning message:
'aggregate = sum' adds the offset, 'aggregate = none' doesn't. 
> 
> y <- rnorm(100, mean = 3 * x1^2, sd = 2)
> DF2 <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)
> amod <- glmboost(y ~ -1 + x1 + I(x1^2), data = DF2, center = FALSE)
> stopifnot(ncol(predict(amod, which="x1")) == 2 && all(rowSums(predict(amod, which="x1")) + attr(coef(amod), "offset") - predict(amod) < sqrt(.Machine$double.eps)))
> 
> 
> amod <- glmboost(y ~ 1+ x1 + x2, data = DF, center = FALSE)
> pr1 <- predict(amod, aggre = "sum", which= 1:2)
> foo <- DF
> foo$x2 <- 0
> pr2 <- predict(amod, aggre = "sum", newdata=foo)
> stopifnot(rowSums(pr1) + attr(coef(amod),"offset") - pr2 < sqrt(.Machine$double.eps))
> newData <- as.data.frame(rbind(mean(DF)[-1], mean(DF)[-2]+1*sd(DF)[-1]))
Warning messages:
1: mean(<data.frame>) is deprecated.
 Use colMeans() or sapply(*, mean) instead. 
2: mean(<data.frame>) is deprecated.
 Use colMeans() or sapply(*, mean) instead. 
3: sd(<data.frame>) is deprecated.
 Use sapply(*, sd) instead. 
> if (!is.list(pr <- predict(amod, newdata=newData, which=1:2)))
+     warning("predict(amod, newdata=newData, which=1:2) does not return a list") # no list but a matrix is returned!
Warning message:
predict(amod, newdata=newData, which=1:2) does not return a list 
> stopifnot(is.list(pr <- predict(amod, newdata=newData, aggregate="cumsum", which=1:2)))
> amod[10]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ 1 + x1 + x2, data = DF, center = FALSE)


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 10 
Step size:  0.1 
Offset:  0.05114654 

Coefficients: 
      x1 
2.095157 
attr(,"offset")
[1] 0.05114654

> pr <- predict(amod, which=1:3)
> stopifnot(ncol(pr) == 3 || all(pr[,c(1,ncol)] == 0))
> amod[100]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ 1 + x1 + x2, data = DF, center = FALSE)


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  0.05114654 

Coefficients: 
(Intercept)          x1          x2 
 0.02278342  3.20585948 -0.24376100 
attr(,"offset")
[1] 0.05114654

> 
> # compare predictions with gamboost
> mod1 <- glmboost(y ~ -1 + x1 + x2 + x3, data = DF, center=FALSE)
> mod2 <- gamboost(y ~ x1 + x2 + x3, data = DF, baselearner= function(x) bols(x, intercept=FALSE))
> pr1_2 <- predict(mod2, aggre = "cumsum")
> pr2_2 <- predict(mod2, aggre = "none")
> pr3_2 <- predict(mod2, aggre = "sum")
> 
> stopifnot(max(abs(predict(mod1) - predict(mod2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(mod1, aggre = "none") - predict(mod2, aggre = "none"))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(mod1, aggre = "cumsum") - predict(mod2, aggre = "cumsum"))) < sqrt(.Machine$double.eps))
> 
> # check type argument
> set.seed(1907)
> x1 <- rnorm(100)
> p <- 1/(1 + exp(- 3 * x1))
> y <- as.factor(runif(100) < p)
> DF <- data.frame(y = y, x1 = x1)
> 
> logitBoost <- glmboost(y ~ x1, family = Binomial(), center = FALSE,
+                  data = DF,  control=boost_control(mstop=5000))
> logit <- glm(y ~ x1, data=DF, family=binomial)
> stopifnot(coef(logitBoost)[2]*2 - coef(logit)[2] < 1e-5) # * 2 as we use y = {-1, 1}
> 
> pr <- predict(logitBoost)
> pr2 <- predict(logit)
> stopifnot(pr * 2 - pr2 < 1e-5)  # * 2 as we use y = {-1, 1}
> 
> pr <- predict(logitBoost, type="class")
> pr2 <- predict(logit, type="response") > 0.5
> foo <- table(pr, pr2)
> stopifnot(foo[1,2] + foo[2,1] == 0)
> 
> pr <- predict(logitBoost, type="response")
> pr2 <- predict(logit, type="response")
> stopifnot(pr - pr2 < sqrt(.Machine$double.eps))
> 
> ### coefficients:
> set.seed(1907)
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> x3 <- rnorm(100)
> y <- rnorm(100, mean = 3 * x1, sd = 2)
> DF <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)
> amod <- glmboost(y ~ x1 + x2 + x3, data = DF, center = FALSE)
> 
> stopifnot(length(coef(amod)) == 4)
> amod[10]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = y ~ x1 + x2 + x3, data = DF, center = FALSE)


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 10 
Step size:  0.1 
Offset:  0.05114654 

Coefficients: 
      x1 
2.095157 
attr(,"offset")
[1] 0.05114654

> stopifnot(length(coef(amod)) == 1)
> stopifnot(length(coef(amod, which=1:3)) == 3)
> 
> ### check with logical design matrices
> n <- 1000
> p <- 10
> X <- matrix(as.logical(rbinom(n * p, size = 1, prob = 0.5)), nrow = n)
> y <- rnorm(n)
> 
> mod <- glmboost(x = X, y = y)
Warning message:
In glmboost.matrix(x = X, y = y) :
  model with centered covariates does not contain intercept
> 
> Xd <- X
> storage.mode(Xd) <- "double"
> modd <- glmboost(x = Xd, y = y)
Warning message:
In glmboost.matrix(x = Xd, y = y) :
  model with centered covariates does not contain intercept
> stopifnot(all.equal(coef(modd), coef(mod)))
> 
> ### probit models
> set.seed(29)
> n <- 1000
> x <- runif(n, min = -2, max = 2)
> y <- factor(rbinom(n, size = 1, prob = pnorm(3 * x)))
> table(y)
y
  0   1 
497 503 
> mod <- glm(y ~ x, family = binomial(link = "probit"))
> coef(mod)
(Intercept)           x 
 0.09253477  3.10331037 
> 
> mod2 <- glmboost(y ~ x, family = Binomial(link = "probit"),
+                  control = boost_control(nu = 0.5, mstop = 1000))
> coef(mod2, off2int = TRUE)
(Intercept)           x 
 0.09211252  3.10050675 
> stopifnot(all.equal(round(coef(mod), 2), round(coef(mod2, off2int = TRUE), 2)))
> 
> data("GlaucomaM", package = "ipred")
> coef(mod3 <- glm(Class ~ varg, data = GlaucomaM, family = binomial(link = "probit")))
(Intercept)        varg 
  -1.857396    6.689428 
> coef(mod4 <- glmboost(Class ~ varg, data = GlaucomaM, family = Binomial(link = "probit"))[1000])
(Intercept)        varg 
  -1.857388    6.689372 
attr(,"offset")
[1] 0
> stopifnot(all.equal(round(coef(mod3), 3), round(coef(mod4, off2int = TRUE), 3)))
> 
> 
